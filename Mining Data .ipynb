{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Data (Text)\n",
    "Created by Atmam Al Faruq\n",
    "\n",
    "Proses pertama dalam memulai membangun sistem yang menunjang NLP diperlukan data yang cukup. \n",
    "Perlu pengambil dan manajamen data dalam proses membangun sistem tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "pada proses ini akan dicoba membagi sebuah kalimat menjadi beberapa kata yang membangun kalimat tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jika', 'engkau', 'tidak', 'sanggup', 'menahan', 'lelahnya', 'belajar', ',', 'maka', 'bersiaplah', 'engkau', 'dengan', 'perihnya', 'kebodohan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.corpus\n",
    "nltk.download('punkt')\n",
    "\n",
    "kalimat = \"Jika engkau tidak sanggup menahan lelahnya belajar, maka bersiaplah engkau dengan perihnya kebodohan\"\n",
    "tokens = word_tokenize(kalimat)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menemukan kata yang berbeda ( frequency distinct )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'engkau': 2, 'Jika': 1, 'tidak': 1, 'sanggup': 1, 'menahan': 1, 'lelahnya': 1, 'belajar': 1, ',': 1, 'maka': 1, 'bersiaplah': 1, ...})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(tokens)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('engkau', 2), ('Jika', 1), ('tidak', 1), ('sanggup', 1), ('menahan', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_1 = fdist.most_common(5)\n",
    "fdist_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jika engkau tidak sanggup tahan lelah ajar maka siap engkau dengan perih bodoh'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "stemmer.stem(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pelajaran : ajar\n",
      "pelajar : ajar\n",
      "pengajar : ajar\n"
     ]
    }
   ],
   "source": [
    "akar_kata = [\"pelajaran\",\"pelajar\",\"pengajar\"]\n",
    "\n",
    "for kata in akar_kata:\n",
    "    print(kata+\" : \"+stemmer.stem(kata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pelajaran : pelaj\n",
      "pelajar : pelaj\n",
      "pengajar : pengaj\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "for kata in akar_kata:\n",
    "    print(kata+\" : \"+lst.stem(kata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming english word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waited : wait\n",
      "waiting : wait\n",
      "waits : wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pst = PorterStemmer()\n",
    "\n",
    "word = [\"waited\",\"waiting\",\"waits\"]\n",
    "\n",
    "for stm_word in word:\n",
    "    print(stm_word+\" : \"+pst.stem(stm_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving : giv\n",
      "given : giv\n",
      "given : giv\n",
      "gave : gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "word = [\"giving\", \"given\", \"given\", \"gave\"]\n",
    "\n",
    "for stm_word in word:\n",
    "    print(stm_word+\" : \"+lst.stem(stm_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic_danish_dutch_english_finnish_french_german_hungarian_italian_norwegian_porter_portuguese_romanian_russian_spanish_swedish\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "print(\"_\".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving : gave\n",
      "given : gave\n",
      "given : gave\n",
      "gave : gave\n"
     ]
    }
   ],
   "source": [
    "snw = SnowballStemmer(\"english\")\n",
    "\n",
    "for snw_word in word:\n",
    "    print(snw_word+\" : \"+snw.stem(stm_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora : corpus\n",
      "ran : ran\n",
      "rocks : rock\n"
     ]
    }
   ],
   "source": [
    "word = [\"corpora\",\"ran\",\"rocks\"]\n",
    "\n",
    "for lem_word in word:\n",
    "    print(lem_word+\" : \"+lemmatizer.lemmatize(lem_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora : corpora\n",
      "ran : run\n",
      "rocks : rock\n"
     ]
    }
   ],
   "source": [
    "for lem_word in word:\n",
    "    print(lem_word+\" : \"+lemmatizer.lemmatize(lem_word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    " \n",
    "factory = StopWordRemoverFactory()\n",
    "b = factory.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Awal : \n",
      "latih apa yang telah engkau pelajari\n",
      "\n",
      "\n",
      " Setelah proses : \n",
      "latih engkau pelajari\n"
     ]
    }
   ],
   "source": [
    "kalimat_2 = \"Latih apa yang telah engkau pelajari\"\n",
    "kalimat_2 = word_tokenize(kalimat_2.lower())\n",
    "print(\" Awal : \")\n",
    "print(\" \".join(kalimat_2))\n",
    "print(\"\\n\")\n",
    "bahasa_stopwords = [x for x in kalimat_2 if x not in b]\n",
    "print(\" Setelah proses : \")\n",
    "print(\" \".join(bahasa_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "a = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Awal : \n",
      "some television channels reported that police had imposed emergencylaw in some parts of the capital , new delhi , that prohibits gatherings .\n",
      "\n",
      "\n",
      " Setelah proses : \n",
      "television channels reported police imposed emergencylaw parts capital , new delhi , prohibits gatherings .\n"
     ]
    }
   ],
   "source": [
    "kalimat_3 = \"Some television channels reported that police had imposed emergencylaw in some parts of the capital, New Delhi, that prohibits gatherings.\"\n",
    "kalimat_3 = word_tokenize(kalimat_3.lower())\n",
    "print(\" Awal : \")\n",
    "print(\" \".join(kalimat_3))\n",
    "print(\"\\n\")\n",
    "stopwords = [x for x in kalimat_3 if x not in a]\n",
    "print(\" Setelah proses : \")\n",
    "print(\" \".join(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech tagging\n",
    "\n",
    "CC coordinating conjunction\n",
    "CD cardinal digit\n",
    "DT determiner\n",
    "EX existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW foreign word\n",
    "IN preposition/subordinating conjunction\n",
    "JJ adjective 'big'\n",
    "JJR adjective, comparative 'bigger'\n",
    "JJS adjective, superlative 'biggest'\n",
    "LS list marker 1)\n",
    "MD modal could, will\n",
    "NN noun, singular 'desk'\n",
    "NNS noun plural 'desks'\n",
    "NNP proper noun, singular 'Harrison'\n",
    "NNPS proper noun, plural 'Americans'\n",
    "PDT predeterminer 'all the kids'\n",
    "POS possessive ending parent's\n",
    "PRP personal pronoun I, he, she\n",
    "PRP possessive pronoun my, his, hers\n",
    "RB adverb very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP particle give up\n",
    "TO to go 'to' the store.\n",
    "UH interjection errrrrrrrm\n",
    "VB verb, base form take\n",
    "VBD verb, past tense took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle taken\n",
    "VBP verb, sing. present, non-3d take\n",
    "VBZ verb, 3rd person sing. present takes\n",
    "WDT wh-determiner which\n",
    "WP wh-pronoun who, what\n",
    "WP possessive wh-pronoun whose\n",
    "WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno -2] Name or service not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('Critics', 'NNS')],\n",
       " [('say', 'VB')],\n",
       " [('the', 'DT')],\n",
       " [('exclusion', 'NN')],\n",
       " [('of', 'IN')],\n",
       " [('Muslims', 'NNS')],\n",
       " [('violates', 'NNS')],\n",
       " [('India', 'NNP')],\n",
       " [(\"'s\", 'POS')],\n",
       " [('secular', 'NN')],\n",
       " [('constitution', 'NN')],\n",
       " [('by', 'IN')],\n",
       " [('making', 'VBG')],\n",
       " [('religion', 'NN')],\n",
       " [('a', 'DT')],\n",
       " [('basis', 'NN')],\n",
       " [('of', 'IN')],\n",
       " [('citizenship', 'NN')],\n",
       " [('.', '.')]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "kalimat_3 = \"Critics say the exclusion of Muslims violates India's secular constitution by making religion a basis of citizenship.\"\n",
    "tex = word_tokenize(kalimat_3)\n",
    "hasil_pos = []\n",
    "for token in tex:\n",
    "  hasil_pos.append(nltk.pos_tag([token]))\n",
    "\n",
    "hasil_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos tagger yang dengan menggunakan model Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Amir', 'NNP'), ('pergi', 'VB'), ('ke', 'IN'), ('Bandung', 'NNP'), ('dini', 'VB'), ('hari', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "\n",
    "ct = CRFTagger()\n",
    "ct.set_model_file('all_indo_man_tag_corpus_model.crf.tagger')\n",
    "hasil = ct.tag_sents([['Amir','pergi','ke','Bandung','dini','hari']])\n",
    "print(hasil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mengidentifikasi Entitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading maxent_ne_chunker: <urlopen error [Errno -2]\n",
      "[nltk_data]     Name or service not known>\n",
      "[nltk_data] Error loading words: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Minhaj/NNP)\n",
      "  comes/VBZ\n",
      "  from/IN\n",
      "  a/DT\n",
      "  (ORGANIZATION Muslim/NNP)\n",
      "  family/NN\n",
      "  originally/RB\n",
      "  from/IN\n",
      "  (GPE Aligarh/NNP)\n",
      "  in/IN\n",
      "  (GPE Uttar/NNP Pradesh/NNP)\n",
      "  ,/,\n",
      "  (GPE India/NNP)\n",
      "  ./.\n",
      "  His/PRP$\n",
      "  parents/NNS\n",
      "  ,/,\n",
      "  (PERSON Najme/NNP)\n",
      "  and/CC\n",
      "  (PERSON Seema/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "kalimat_4 = \"Minhaj comes from a Muslim family originally from Aligarh in Uttar Pradesh, India. His parents, Najme and Seema.\"\n",
    "\n",
    "from nltk import ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "token = word_tokenize(kalimat_4)\n",
    "tags = nltk.pos_tag(token)\n",
    "chunk = ne_chunk(tags)\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Chunking adalah sebuah proses yang memisahkan dan membagi kalimat dalam sebuah bentuk yang lebih sederhana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('yellow', 'JJ'), ('dog', 'NN')]\n",
      "(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "text = \"We saw the yellow dog\"\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "reg = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "a = nltk.RegexpParser(reg)\n",
    "result = a.parse(tags)\n",
    "print(tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n",
      "(NP the/DT yellow/JJ dog/NN)\n"
     ]
    }
   ],
   "source": [
    "tree = a.parse(result)\n",
    "\n",
    "for subtree in tree.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
