{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Data (Text)\n",
    "Created by Atmam Al Faruq\n",
    "\n",
    "Proses pertama dalam memulai membangun sistem yang menunjang NLP diperlukan data yang cukup. \n",
    "Perlu pengambil dan manajamen data dalam proses membangun sistem tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "pada proses ini akan dicoba membagi sebuah kalimat menjadi beberapa kata yang membangun kalimat tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jika', 'engkau', 'tidak', 'sanggup', 'menahan', 'lelahnya', 'belajar', ',', 'maka', 'bersiaplah', 'engkau', 'dengan', 'perihnya', 'kebodohan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/not/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.corpus\n",
    "nltk.download('punkt')\n",
    "\n",
    "kalimat = \"Jika engkau tidak sanggup menahan lelahnya belajar, maka bersiaplah engkau dengan perihnya kebodohan\"\n",
    "tokens = word_tokenize(kalimat)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menemukan kata yang berbeda ( frequency distinct )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'engkau': 2, 'Jika': 1, 'tidak': 1, 'sanggup': 1, 'menahan': 1, 'lelahnya': 1, 'belajar': 1, ',': 1, 'maka': 1, 'bersiaplah': 1, ...})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(tokens)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('engkau', 2), ('Jika', 1), ('tidak', 1), ('sanggup', 1), ('menahan', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_1 = fdist.most_common(5)\n",
    "fdist_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jika engkau tidak sanggup tahan lelah ajar maka siap engkau dengan perih bodoh'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "stemmer.stem(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pelajaran : ajar\n",
      "pelajar : ajar\n",
      "pengajar : ajar\n"
     ]
    }
   ],
   "source": [
    "akar_kata = [\"pelajaran\",\"pelajar\",\"pengajar\"]\n",
    "\n",
    "for kata in akar_kata:\n",
    "    print(kata+\" : \"+stemmer.stem(kata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pelajaran : pelaj\n",
      "pelajar : pelaj\n",
      "pengajar : pengaj\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "for kata in akar_kata:\n",
    "    print(kata+\" : \"+lst.stem(kata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming english word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waited : wait\n",
      "waiting : wait\n",
      "waits : wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pst = PorterStemmer()\n",
    "\n",
    "word = [\"waited\",\"waiting\",\"waits\"]\n",
    "\n",
    "for stm_word in word:\n",
    "    print(stm_word+\" : \"+pst.stem(stm_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving : giv\n",
      "given : giv\n",
      "given : giv\n",
      "gave : gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "word = [\"giving\", \"given\", \"given\", \"gave\"]\n",
    "\n",
    "for stm_word in word:\n",
    "    print(stm_word+\" : \"+lst.stem(stm_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic_danish_dutch_english_finnish_french_german_hungarian_italian_norwegian_porter_portuguese_romanian_russian_spanish_swedish\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "print(\"_\".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving : gave\n",
      "given : gave\n",
      "given : gave\n",
      "gave : gave\n"
     ]
    }
   ],
   "source": [
    "snw = SnowballStemmer(\"english\")\n",
    "\n",
    "for snw_word in word:\n",
    "    print(snw_word+\" : \"+snw.stem(stm_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/not/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora : corpus\n",
      "ran : ran\n",
      "rocks : rock\n"
     ]
    }
   ],
   "source": [
    "word = [\"corpora\",\"ran\",\"rocks\"]\n",
    "\n",
    "for lem_word in word:\n",
    "    print(lem_word+\" : \"+lemmatizer.lemmatize(lem_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora : corpora\n",
      "ran : run\n",
      "rocks : rock\n"
     ]
    }
   ],
   "source": [
    "for lem_word in word:\n",
    "    print(lem_word+\" : \"+lemmatizer.lemmatize(lem_word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    " \n",
    "factory = StopWordRemoverFactory()\n",
    "b = factory.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Awal : \n",
      "latih apa yang telah engkau pelajari\n",
      "\n",
      "\n",
      " Setelah proses : \n",
      "latih engkau pelajari\n"
     ]
    }
   ],
   "source": [
    "kalimat_2 = \"Latih apa yang telah engkau pelajari\"\n",
    "kalimat_2 = word_tokenize(kalimat_2.lower())\n",
    "print(\" Awal : \")\n",
    "print(\" \".join(kalimat_2))\n",
    "print(\"\\n\")\n",
    "bahasa_stopwords = [x for x in kalimat_2 if x not in b]\n",
    "print(\" Setelah proses : \")\n",
    "print(\" \".join(bahasa_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/not/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "a = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Awal : \n",
      "some television channels reported that police had imposed emergencylaw in some parts of the capital , new delhi , that prohibits gatherings .\n",
      "\n",
      "\n",
      " Setelah proses : \n",
      "television channels reported police imposed emergencylaw parts capital , new delhi , prohibits gatherings .\n"
     ]
    }
   ],
   "source": [
    "kalimat_3 = \"Some television channels reported that police had imposed emergencylaw in some parts of the capital, New Delhi, that prohibits gatherings.\"\n",
    "kalimat_3 = word_tokenize(kalimat_3.lower())\n",
    "print(\" Awal : \")\n",
    "print(\" \".join(kalimat_3))\n",
    "print(\"\\n\")\n",
    "stopwords = [x for x in kalimat_3 if x not in a]\n",
    "print(\" Setelah proses : \")\n",
    "print(\" \".join(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech tagging\n",
    "\n",
    "CC coordinating conjunction\n",
    "CD cardinal digit\n",
    "DT determiner\n",
    "EX existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW foreign word\n",
    "IN preposition/subordinating conjunction\n",
    "JJ adjective 'big'\n",
    "JJR adjective, comparative 'bigger'\n",
    "JJS adjective, superlative 'biggest'\n",
    "LS list marker 1)\n",
    "MD modal could, will\n",
    "NN noun, singular 'desk'\n",
    "NNS noun plural 'desks'\n",
    "NNP proper noun, singular 'Harrison'\n",
    "NNPS proper noun, plural 'Americans'\n",
    "PDT predeterminer 'all the kids'\n",
    "POS possessive ending parent's\n",
    "PRP personal pronoun I, he, she\n",
    "PRP possessive pronoun my, his, hers\n",
    "RB adverb very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP particle give up\n",
    "TO to go 'to' the store.\n",
    "UH interjection errrrrrrrm\n",
    "VB verb, base form take\n",
    "VBD verb, past tense took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle taken\n",
    "VBP verb, sing. present, non-3d take\n",
    "VBZ verb, 3rd person sing. present takes\n",
    "WDT wh-determiner which\n",
    "WP wh-pronoun who, what\n",
    "WP possessive wh-pronoun whose\n",
    "WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/not/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('Critics', 'NNS')],\n",
       " [('say', 'VB')],\n",
       " [('the', 'DT')],\n",
       " [('exclusion', 'NN')],\n",
       " [('of', 'IN')],\n",
       " [('Muslims', 'NNS')],\n",
       " [('violates', 'NNS')],\n",
       " [('India', 'NNP')],\n",
       " [(\"'s\", 'POS')],\n",
       " [('secular', 'NN')],\n",
       " [('constitution', 'NN')],\n",
       " [('by', 'IN')],\n",
       " [('making', 'VBG')],\n",
       " [('religion', 'NN')],\n",
       " [('a', 'DT')],\n",
       " [('basis', 'NN')],\n",
       " [('of', 'IN')],\n",
       " [('citizenship', 'NN')],\n",
       " [('.', '.')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "kalimat_3 = \"Critics say the exclusion of Muslims violates India's secular constitution by making religion a basis of citizenship.\"\n",
    "tex = word_tokenize(kalimat_3)\n",
    "hasil_pos = []\n",
    "for token in tex:\n",
    "  hasil_pos.append(nltk.pos_tag([token]))\n",
    "\n",
    "hasil_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos tagger yang dengan menggunakan model Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Amir', 'NNP'), ('pergi', 'VB'), ('ke', 'IN'), ('Bandung', 'NNP'), ('dini', 'VB'), ('hari', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "\n",
    "ct = CRFTagger()\n",
    "ct.set_model_file('all_indo_man_tag_corpus_model.crf.tagger')\n",
    "hasil = ct.tag_sents([['Amir','pergi','ke','Bandung','dini','hari']])\n",
    "print(hasil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mengidentifikasi Entitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Minhaj/NNP)\n",
      "  comes/VBZ\n",
      "  from/IN\n",
      "  a/DT\n",
      "  (ORGANIZATION Muslim/NNP)\n",
      "  family/NN\n",
      "  originally/RB\n",
      "  from/IN\n",
      "  (GPE Aligarh/NNP)\n",
      "  in/IN\n",
      "  (GPE Uttar/NNP Pradesh/NNP)\n",
      "  ,/,\n",
      "  (GPE India/NNP)\n",
      "  ./.\n",
      "  His/PRP$\n",
      "  parents/NNS\n",
      "  ,/,\n",
      "  (PERSON Najme/NNP)\n",
      "  and/CC\n",
      "  (PERSON Seema/NNP)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/not/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/not/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "kalimat_4 = \"Minhaj comes from a Muslim family originally from Aligarh in Uttar Pradesh, India. His parents, Najme and Seema.\"\n",
    "\n",
    "from nltk import ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "token = word_tokenize(kalimat_4)\n",
    "tags = nltk.pos_tag(token)\n",
    "chunk = ne_chunk(tags)\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Chunking adalah sebuah proses yang memisahkan dan membagi kalimat dalam sebuah bentuk yang lebih sederhana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('yellow', 'JJ'), ('dog', 'NN')]\n",
      "(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "text = \"We saw the yellow dog\"\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "reg = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "a = nltk.RegexpParser(reg)\n",
    "result = a.parse(tags)\n",
    "print(tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n",
      "(NP the/DT yellow/JJ dog/NN)\n"
     ]
    }
   ],
   "source": [
    "tree = a.parse(result)\n",
    "\n",
    "for subtree in tree.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6828c246cf50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text mining with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob as tb\n",
    "\n",
    "blob = tb(\"if you are not capable of the learning process, then accept the bitterness of ignorance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning process\n"
     ]
    }
   ],
   "source": [
    "for np in blob.noun_phrases:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if IN\n",
      "you PRP\n",
      "are VBP\n",
      "not RB\n",
      "capable JJ\n",
      "of IN\n",
      "the DT\n",
      "learning NN\n",
      "process NN\n",
      "then RB\n",
      "accept IN\n",
      "the DT\n",
      "bitterness NN\n",
      "of IN\n",
      "ignorance NN\n"
     ]
    }
   ],
   "source": [
    "for words, tags in blob.tags:\n",
    "    print(words,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sanctuaries'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('sanctuary')\n",
    "w.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learnings\n",
      "processes\n",
      "bitternesses\n",
      "ignorances\n"
     ]
    }
   ],
   "source": [
    "for word, pos in blob.tags:\n",
    "    if pos == 'NN':\n",
    "        print(word.pluralize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sanctuary'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.lemmatize(\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you']\n",
      "['you', 'are']\n",
      "['are', 'not']\n",
      "['not', 'capable']\n",
      "['capable', 'of']\n",
      "['of', 'the']\n",
      "['the', 'learning']\n",
      "['learning', 'process']\n",
      "['process', 'then']\n",
      "['then', 'accept']\n",
      "['accept', 'the']\n",
      "['the', 'bitterness']\n",
      "['bitterness', 'of']\n",
      "['of', 'ignorance']\n"
     ]
    }
   ],
   "source": [
    "for ngram in blob.ngrams(2):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you are not capable of the learning process, then accept the bitterness of ignorance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.1, subjectivity=0.4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(blob)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"if you are not capable of the learning process, then accept the bitterness of ignorance\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_2 = tb(\"if y are not capab of the learning process, then accept the bitterness of ignorance\")\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cap', 0.2545931758530184),\n",
       " ('canal', 0.1889763779527559),\n",
       " ('capable', 0.14173228346456693),\n",
       " ('papa', 0.13123359580052493),\n",
       " ('cab', 0.08661417322834646),\n",
       " ('japan', 0.05511811023622047),\n",
       " ('caps', 0.049868766404199474),\n",
       " ('carpal', 0.02099737532808399),\n",
       " ('campan', 0.02099737532808399),\n",
       " ('arab', 0.013123359580052493),\n",
       " ('cava', 0.007874015748031496),\n",
       " ('cape', 0.007874015748031496),\n",
       " ('crab', 0.005249343832020997),\n",
       " ('cabal', 0.005249343832020997),\n",
       " ('papal', 0.0026246719160104987),\n",
       " ('cavae', 0.0026246719160104987),\n",
       " ('caper', 0.0026246719160104987),\n",
       " ('caleb', 0.0026246719160104987)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_2.words[4].spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = [\n",
    "('Tom Holland is a terrible spiderman.','pos'),\n",
    "('a terrible Javert (Russell Crowe) ruined Les Miserables for me...','pos'),\n",
    "('The Dark Knight Rises is the greatest superhero movie ever!','neg'),\n",
    "('Fantastic Four should have never been made.','pos'),\n",
    "('Wes Anderson is my favorite director!','neg'),\n",
    "('Captain America 2 is pretty awesome.','neg'),\n",
    "('Let\\s pretend \"Batman and Robin\" never happened..','pos'),\n",
    "]\n",
    "testing = [\n",
    "('Superman was never an interesting character.','pos'),\n",
    "('Fantastic Mr Fox is an awesome film!','neg'),\n",
    "('Dragonball Evolution is simply terrible!!','pos')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import classifiers\n",
    "classifier = classifiers.NaiveBayesClassifier(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decision tree classifier\n",
    "dt_classifier = classifiers.DecisionTreeClassifier(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Most Informative Features\n",
      "            contains(is) = True              neg : pos    =      2.9 : 1.0\n",
      "      contains(terrible) = False             neg : pos    =      1.8 : 1.0\n",
      "             contains(a) = False             neg : pos    =      1.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print (classifier.accuracy(testing))\n",
    "classifier.show_informative_features(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "blob = tb('the weather is terrible!', classifier=classifier)\n",
    "print (blob.classify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
